# -*- coding: utf-8 -*-
"""CarPricePrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/103VTbLAh3LBCKBVi-Rjcrldnd9b7ODD3
"""

# Import Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error

# Load Dataset
car = pd.read_csv('/content/quikr_car.csv')  # Replace with actual dataset path

car.head()

#Remove NaN values
car = car.dropna()  # Drops rows with any NaN values
car = car.dropna(axis=1)  # Drops columns with any NaN values

# Convert all values to string first
car['kms_driven'] = car['kms_driven'].astype(str)

# Remove commas, strip spaces, remove 'kms', and convert to integer
car['kms_driven'] = car['kms_driven'].str.replace(',', '')  # Remove commas
car['kms_driven'] = car['kms_driven'].str.strip()           # Remove leading/trailing spaces
car['kms_driven'] = car['kms_driven'].str.replace('kms', '')  # Remove 'kms'
car['kms_driven'] = car['kms_driven'].astype(int)            # Convert to integer



# Convert the cleaned strings to integer
car['kms_driven'] = car['kms_driven'].astype(int)

# Print the updated dataframe
print(car)

car.shape

car.info()

backup=car.copy()

print(car.dtypes)

"""Data Cleaning"""

# Filter out rows where Price is 'Ask For Price'
car = car[car['Price'] != 'Ask For Price']

# Convert Price to string, remove commas, and convert to integer
car['Price'] = car['Price'].astype(str)  # Convert to string first
car['Price'] = car['Price'].str.replace(',', '')  # Remove commas
car['Price'] = car['Price'].astype(int)  # Convert to integer

# Print the updated DataFrame
print(car)

# kms_driven has nan values and two rows have 'Petrol' in them

car['kms_driven']=car['kms_driven'].astype(int)

#Fuel type has Nan values
car=car[~car['fuel_type'].isna()]

car.shape

#Company does not need any cleaning now. Changing car names. Keeping only the first three word
car['name']=car['name'].str.split().str.slice(start=0,stop=3).str.join(' ')
#Resetting the index of the final cleaned data
car=car.reset_index(drop=True)

#Cleaned data
car

car.info()

car.to_csv('Cleaned_Car_data.csv')

car.describe(include='all')

car.isnull().sum()

car.describe()

sns.pairplot(car, hue='year', palette='Set2')

plt.subplots(figsize=(15,7))
ax=sns.boxplot(x='company',y='Price',data=car,palette="Set2")
ax.set_xticklabels(ax.get_xticklabels(),rotation=40,ha='right')
plt.show()

car.columns

#Checking relationship of Company with Price
car['company'].unique()

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Convert 'year' to numeric if it is not already
car['year'] = pd.to_numeric(car['year'], errors='coerce')

# Sort the data by year
car = car.sort_values('year')

# Checking relationship of Year with Price
plt.subplots(figsize=(15, 10))
ax = sns.swarmplot(x='year', y='Price', data=car, palette="Set2")

# Rotate the x-axis labels for better readability
ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha='right')

plt.tight_layout()
plt.show()

#Checking relationship of kms_driven with Price
sns.relplot(x='kms_driven',y='Price',data=car,height=5,aspect=1.5,palette="rocket")
plt.show()

#Checking relationship of Fuel Type with Price
plt.subplots(figsize=(10,7))
sns.boxplot(x='fuel_type',y='Price',data=car,palette="Set2")

#Relationship of Price with FuelType, Year and Company mixed
ax=sns.relplot(x='company',y='Price',data=car,hue='fuel_type',size='year',height=7,aspect=2,palette="Set2")
ax.set_xticklabels(rotation=40,ha='right')

"""Training Data"""

#Extracting Training Data
X=car[['name','company','year','kms_driven','fuel_type']]
X

Y=car['Price']
Y.shape
Y.head()

"""Applying Train Test split"""

from sklearn.model_selection import train_test_split
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2)

"""Applying Linear Regression"""

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import make_column_transformer
from sklearn.pipeline import make_pipeline
from sklearn.metrics import r2_score

"""Creating an OneHotEncoder object to contain all the possible categories"""

ohe=OneHotEncoder()
ohe.fit(X[['name','company','fuel_type']])

"""Creating a column transformer to transform categorical columns"""

column_trans=make_column_transformer((OneHotEncoder(categories=ohe.categories_),['name','company','fuel_type']),
                                    remainder='passthrough')

"""**Linear Regression Model**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import OneHotEncoder

# Assuming 'car' is your DataFrame and 'Price' is the target variable
X = car[['name','company','year','fuel_type']]
#X = car[['company','year','kms_driven','fuel_type']]  # Input features including the categorical 'company'
Y = car['Price']  # Target variable

# One-Hot Encoding the categorical variable(s) using pandas' get_dummies
X_encoded = pd.get_dummies(X, drop_first=True)  # drop_first=True avoids multicollinearity in linear models

# Split the data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X_encoded, Y, test_size=0.2, random_state=42)

# Create and fit a Linear Regression model
lr = LinearRegression()
lr.fit(X_train, Y_train)

# Make predictions
Y_pred = lr.predict(X_test)

# Evaluate the model
from sklearn.metrics import r2_score
r2 = r2_score(Y_test, Y_pred)
print("R² score:", r2)

train_pred = lr.predict(X_train)
train_r2 = r2_score(Y_train, train_pred)
print("Training R² score:", train_r2)

"""pipeline and Fitting the **model**"""

pipe=make_pipeline(column_trans,lr)

pipe.fit(X_train,Y_train)

scores=[]
for i in range(1000):
    X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.1,random_state=i)
    lr=LinearRegression()
    pipe=make_pipeline(column_trans,lr)
    pipe.fit(X_train,Y_train)
    Y_pred=pipe.predict(X_test)
    scores.append(r2_score(Y_test,Y_pred))

np.argmax(scores)

scores[np.argmax(scores)]

print(X_test.columns)
print(len(X_test.columns))

pipe.predict(pd.DataFrame(columns=X_test.columns,data=np.array(['Maruti Suzuki Swift','Maruti',2019,'Petrol']).reshape(1,4)))

X_train.columns

Y_train=car['Price']

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Define features and target
X = car[['name', 'company', 'year', 'kms_driven', 'fuel_type']]  # Features
Y = car['Price']  # Target

# Define the columns that are categorical
categorical_columns = ['name', 'company', 'fuel_type']

# Create the ColumnTransformer to handle encoding of categorical variables
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)
    ],
    remainder='passthrough'  # Leave 'year' and 'kms_driven' untouched
)

# Create a pipeline that applies preprocessing and then fits Logistic Regression
pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', LogisticRegression())
])

# Split the data into training and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Fit the pipeline
pipe.fit(X_train, Y_train)

# Predict on the test set
Y_pred = pipe.predict(X_test)

# Calculate the accuracy (note: Logistic Regression is typically used for classification)
accuracy = accuracy_score(Y_test, Y_pred)
print(f"Accuracy of Logistic Regression: {accuracy*1000:0.2f}%")
accuracy = accuracy_score(Y_test, Y_pred)
print("Accuracy of Logistic Regression:", accuracy)

"""Random Forest"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline

# Define features and target
X = car[['name', 'company', 'year', 'kms_driven', 'fuel_type']]  # Features
Y = car['Price']  # Target

# Define the columns that are categorical
categorical_columns = ['name', 'company', 'fuel_type']

# Create the ColumnTransformer to handle encoding of categorical variables
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)
    ],
    remainder='passthrough'  # Leave 'year' and 'kms_driven' as they are
)

# Create a pipeline that applies preprocessing and then fits Random Forest Regressor
pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))
])

# Split the data into training and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Fit the pipeline
pipe.fit(X_train, Y_train)

# Predict on the test set
Y_pred = pipe.predict(X_test)

# Evaluate the model
mse = mean_squared_error(Y_test, Y_pred)
r2 = r2_score(Y_test, Y_pred)

print(f"Mean Squared Error (MSE): {mse}")
print(f"R² Score: {r2}")

"""Decision Tree"""

from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.pipeline import Pipeline

# Define features and target
X = car[['name', 'company', 'year', 'kms_driven', 'fuel_type']]  # Features
Y = car['Price']  # Target

# Define the columns that are categorical
categorical_columns = ['name', 'company', 'fuel_type']

# Create the ColumnTransformer to handle encoding of categorical variables
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)
    ],
    remainder='passthrough'  # Leave 'year' and 'kms_driven' untouched
)

# Create a pipeline that applies preprocessing and then fits Decision Tree Regressor
pipe = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', DecisionTreeRegressor(random_state=42))  # You can specify max_depth, min_samples_split, etc. if needed
])

# Split the data into training and test sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

# Fit the pipeline
pipe.fit(X_train, Y_train)

# Predict on the test set
Y_pred = pipe.predict(X_test)

# Evaluate the model
mse = mean_squared_error(Y_test, Y_pred)
r2 = r2_score(Y_test, Y_pred)

print(f"Mean Squared Error (MSE): {mse}")
print(f"R² Score: {r2}")